{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eaab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "single_layer = True #whether to fit single layer or full structure\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cef35f",
   "metadata": {},
   "source": [
    "# Auxillary functions initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def u2mzi(\n",
    "    dim, m, n, theta, phi,\n",
    "    lp=torch.tensor(1, dtype=torch.complex128),\n",
    "    lc=torch.tensor(1, dtype=torch.complex128),\n",
    "):\n",
    "    assert m < n < dim\n",
    "    mat = torch.eye(dim, dtype=torch.complex128).to(theta.device)\n",
    "    sqrt_lp = torch.sqrt(lp).to(phi.device)\n",
    "    sqrt_lc = torch.sqrt(lc).to(phi.device)\n",
    "    phase = torch.exp(1j * theta)\n",
    "\n",
    "    mat[m, m] = sqrt_lp * phase * torch.sin(phi)\n",
    "    mat[m, n] = sqrt_lc * phase * torch.cos(phi)\n",
    "    mat[n, m] = sqrt_lc * torch.cos(phi)\n",
    "    mat[n, n] = -sqrt_lp * torch.sin(phi)\n",
    "    return mat\n",
    "\n",
    "def angle_diff(\n",
    "        comp_src, comp_dst, offset=0, tolerance=torch.tensor(1e-6),\n",
    "        wrap=True, to_degree=False\n",
    "    ):\n",
    "    zero_src = torch.abs(comp_src) <= tolerance\n",
    "    zero_dst = torch.abs(comp_dst) <= tolerance\n",
    "\n",
    "    rad = torch.where(\n",
    "        zero_src & zero_dst, torch.tensor(0.),\n",
    "        torch.where(\n",
    "            zero_src, torch.angle(comp_dst),\n",
    "            torch.where(\n",
    "                zero_dst, -torch.angle(comp_src),\n",
    "                torch.angle(comp_src) - torch.angle(comp_dst)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    rad = torch.remainder(rad + offset, 2 * torch.pi) if wrap else rad + offset\n",
    "    return torch.rad2deg(rad) if to_degree else rad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6339ee",
   "metadata": {},
   "source": [
    "# Reck unitary class initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReckUnitaryLayer(nn.Module):\n",
    "    def __init__(self, dim=4, lp_db=0, lc_db=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num = int(dim * (dim - 1) / 2)\n",
    "\n",
    "        self.lp = 10 ** (lp_db / 10)\n",
    "        self.lc = 10 ** (lc_db / 10)\n",
    "\n",
    "        self.lp = torch.tensor(self.lp, dtype=torch.float64)\n",
    "        self.lc = torch.tensor(self.lc, dtype=torch.float64)\n",
    "\n",
    "        self.phis = nn.Parameter(\n",
    "            torch.rand(self.num, dtype=torch.float64)*2*torch.pi\n",
    "        )\n",
    "        self.thetas = nn.Parameter(\n",
    "            torch.rand(self.num-3, dtype=torch.float64)*2*torch.pi\n",
    "        )\n",
    "        self.thetas_constant = torch.zeros(3, dtype=torch.float64, device=self.phis.device)\n",
    "\n",
    "        self.register_buffer(\"alphas\", torch.zeros(self.dim, dtype=torch.float64))\n",
    "        self.q_list, self.p_list = self.get_q_p()\n",
    "\n",
    "    def get_q_p(self):\n",
    "        q_list, p_list = [], []\n",
    "        for p in range(1, self.dim):\n",
    "            for q in range(p):\n",
    "                q_list.append(q)\n",
    "                p_list.append(p)\n",
    "        return q_list, p_list\n",
    "    \n",
    "    def theta_concat(self):\n",
    "        return torch.cat([self.thetas_constant.to(self.thetas.device), self.thetas])\n",
    "    \n",
    "    def reconstruct(self, phis, thetas, alphas):\n",
    "        self.mat = torch.diag(torch.exp(-1j * alphas)).to(torch.complex128)\n",
    "        for q, p, theta, phi in zip(\n",
    "            self.q_list, self.p_list,\n",
    "            thetas.flip(0), phis.flip(0)\n",
    "        ):\n",
    "            self.mat = self.mat @ u2mzi(\n",
    "                self.dim, q, p, theta, phi, lp=self.lp, lc=self.lc\n",
    "            ).conj().T\n",
    "        return self.mat    \n",
    "\n",
    "    def decompose(self, u):\n",
    "        index = 0\n",
    "        phis = torch.zeros(self.num, device=u.device)\n",
    "        thetas = torch.zeros(self.num, device=u.device)\n",
    "        alphas = torch.zeros(self.dim, device=u.device)\n",
    "        mat = torch.clone(u).to(torch.complex128)\n",
    "        for x, y in zip(self.p_list[::-1], self.q_list[::-1]):\n",
    "            phis[index] = torch.atan2(\n",
    "                torch.abs(mat[x, x]), torch.abs(mat[x, y])\n",
    "            )\n",
    "            thetas[index] = angle_diff(mat[x, x], mat[x, y], offset=torch.pi)\n",
    "            mat = mat @ u2mzi(\n",
    "                self.dim, y, x, thetas[index], phis[index],\n",
    "            )\n",
    "            index += 1\n",
    "\n",
    "        for i in range(self.dim):\n",
    "            alphas[i] = -torch.angle(mat[i, i])\n",
    "\n",
    "        return phis, thetas, alphas\n",
    "\n",
    "    def forward(self):\n",
    "        return self.reconstruct(self.phis, self.theta_concat(), self.alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747d89d",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data_no_current = []\n",
    "\n",
    "if single_layer:\n",
    "    file_path = \"Calibration Example_915\"\n",
    "else:\n",
    "    file_path = \"full-structure-data\"\n",
    "\n",
    "for ch in range(4):\n",
    "    for H in range(3):\n",
    "        with open(\n",
    "            # start with +1 in case of a single layer and with +5,+9 or +14 to the incdeces in case of the full structure\n",
    "            file_path+f\"/ch{ch + 1}_H{H + 1}.txt\"\n",
    "        ) as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    row = [float(num) for num in line.split()]\n",
    "                    data.append([ch,H] + row)\n",
    "                    if row[0] == 0:\n",
    "                        data_no_current.append([ch,H] + row)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575df21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data, columns=['in_ch', 'heater', 'current', 'ch1', 'ch2', 'ch3', 'ch4'],\n",
    ")\n",
    "\n",
    "\"\"\"Normalize the channels to sum to 1 for each row.\"\"\"\n",
    "cols_to_normalize = ['ch1', 'ch2', 'ch3', 'ch4']\n",
    "df[cols_to_normalize] = df[cols_to_normalize].div(\n",
    "    df[cols_to_normalize].sum(axis=1), axis=0\n",
    ")\n",
    "\n",
    "if single_layer:\n",
    "    current_coefficient = 10**(-3)\n",
    "else:\n",
    "    current_coefficient = 10**1\n",
    "df[\"power\"] = (current_coefficient * df[\"current\"]) ** 2\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data tensors\n",
    "ind = torch.tensor(df[\"heater\"].values) \n",
    "\n",
    "x_all = torch.zeros((len(df), 3), dtype=torch.float64)\n",
    "x_all[torch.arange(len(data)), ind] = torch.tensor(df[\"power\"].values)\n",
    "\n",
    "target = [np.expand_dims(df[i].values, axis=1) for i in ['ch1', 'ch2', 'ch3', 'ch4']] \n",
    "target = np.concatenate(target, axis=1)\n",
    "target = torch.tensor(target, dtype=torch.float64)\n",
    "\n",
    "indx = torch.tensor(df[\"in_ch\"].values, device=device)\n",
    "indx = indx.unsqueeze(-1).unsqueeze(-1)\n",
    "indx = indx.expand(-1, -1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a883ac",
   "metadata": {},
   "source": [
    "# Phase shift layer initialization - credits to Gemini for implementation of physical constrains (see project description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd915a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigmoid(x):\n",
    "    x = np.clip(x, 1e-6, 1 - 1e-6)\n",
    "    return np.log(x / (1 - x))\n",
    "\n",
    "class Structure(nn.Module):\n",
    "    def __init__(self, dim=4, lp_db=0, lc_db=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.m1 = ReckUnitaryLayer(dim, lp_db, lc_db)\n",
    "        self.m2 = ReckUnitaryLayer(dim, lp_db, lc_db)\n",
    "\n",
    "        # --- 1. Unitary Initialization (Mixing) ---\n",
    "        with torch.no_grad():\n",
    "            self.m1.phis.data.fill_(torch.pi / 4.0) \n",
    "            self.m2.phis.data.fill_(torch.pi / 4.0)\n",
    "            self.m1.thetas.data.fill_(0.0)\n",
    "            self.m2.thetas.data.fill_(0.0)\n",
    "\n",
    "        # --- 2. Constrained Alpha Initialization ---\n",
    "        # Targets: ~9.5, ~8.5, ~6.5\n",
    "        r0 = inv_sigmoid(0.95)\n",
    "        r1 = inv_sigmoid(0.7894)\n",
    "        r2 = inv_sigmoid(0.5294)\n",
    "        self.raw_diag = nn.Parameter(torch.tensor([r0, r1, r2], dtype=torch.float64))\n",
    "        \n",
    "        # Initialize crosstalk near 0\n",
    "        self.raw_matrix = nn.Parameter(torch.full((3, 3), inv_sigmoid(0.4), dtype=torch.float64))\n",
    "        \n",
    "        # --- 3. Physical Corrections Parameters ---\n",
    "        \n",
    "        # Linear Phase Intercept\n",
    "        self.h_0 = nn.Parameter(torch.zeros(3, dtype=torch.float64))\n",
    "        \n",
    "        # Quadratic Phase Coefficient (h = h0 + a*P + beta*P^2)\n",
    "        # Initialized to 0, allowed to learn small non-linearities\n",
    "        self.beta = nn.Parameter(torch.zeros((3,3), dtype=torch.float64))\n",
    "\n",
    "        # Input/Output Coupling Efficiencies (Log space for stability)\n",
    "        # Initialize to 0 (which means efficiency = exp(0) = 1.0)\n",
    "        self.in_log_eff = nn.Parameter(torch.zeros(dim, dtype=torch.float64))\n",
    "        self.out_log_eff = nn.Parameter(torch.zeros(dim, dtype=torch.float64))\n",
    "        \n",
    "        # Background Noise / Dark Current (Add to final intensity)\n",
    "        # Initialize to a small value (e.g., 0.01)\n",
    "        self.background = nn.Parameter(torch.full((dim,), 0.001, dtype=torch.float64))\n",
    "\n",
    "    def get_constrained_alpha(self):\n",
    "        sigmoid = torch.sigmoid\n",
    "        \n",
    "        # Diagonals\n",
    "        a00 = 10.0 * sigmoid(self.raw_diag[0]) + 1e-6\n",
    "        a11 = a00 * (0.5 + 0.5 * sigmoid(self.raw_diag[1]))\n",
    "        a22 = a11 * (0.5 + 0.5 * sigmoid(self.raw_diag[2]))\n",
    "        diag_values = torch.stack([a00, a11, a22])\n",
    "        \n",
    "        # Off-Diagonals\n",
    "        lower_bound = -4.0 \n",
    "        upper_bound = a22 \n",
    "        constrained_matrix = lower_bound + (upper_bound - lower_bound) * sigmoid(self.raw_matrix)\n",
    "        \n",
    "        eye = torch.eye(3, device=self.raw_diag.device, dtype=self.raw_diag.dtype)\n",
    "        alpha = (1 - eye) * constrained_matrix + torch.diag_embed(diag_values)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, x, indx, bottom_heaters=True):\n",
    "        alpha = self.get_constrained_alpha()\n",
    "        \n",
    "        # Handle Phase Shifts (Linear + Quadratic)\n",
    "        # Alpha needs to be flipped for your specific heater layout\n",
    "        if bottom_heaters:\n",
    "            alpha_val = alpha.to(x.device)\n",
    "            beta_val = self.beta.to(x.device)\n",
    "        else:\n",
    "            alpha_val = alpha.to(x.device).flip([0,1])\n",
    "            beta_val = self.beta.to(x.device).flip([0,1])\n",
    "        # h = h0 + alpha*P + beta*P^2\n",
    "        h_0_val = self.h_0.to(x.device)\n",
    "        \n",
    "        # Linear term: (alpha @ x.T).T\n",
    "        # Quadratic term: (beta @ (x**2).T).T\n",
    "        h_list = h_0_val + (alpha_val @ x.T).T + (beta_val @ (x**2).T).T\n",
    "        \n",
    "        # Construct Phase Matrix\n",
    "        if bottom_heaters:\n",
    "            h_diag = torch.cat([\n",
    "                torch.ones((x.shape[0], 1), dtype=torch.complex128, device=x.device),\n",
    "                torch.exp(1j * h_list),\n",
    "            ], dim=1)\n",
    "        else:\n",
    "            h_diag = torch.cat([\n",
    "                torch.exp(1j * h_list),\n",
    "                torch.ones((x.shape[0], 1), dtype=torch.complex128, device=x.device)\n",
    "            ], dim=1)\n",
    "\n",
    "        # Get Unitaries\n",
    "        m1 = self.m1()\n",
    "        m2 = self.m2()\n",
    "        \n",
    "        # --- Apply Coupling Efficiencies ---\n",
    "        # Diagonal matrices for input/output scaling\n",
    "        # We work in amplitude domain, so efficiency scales amplitude by sqrt(eff)\n",
    "        # But since parameters are arbitrary log-gains, we just apply exp(log_eff)\n",
    "        in_gain = torch.diag(torch.exp(self.in_log_eff))\n",
    "        out_gain = torch.diag(torch.exp(self.out_log_eff))\n",
    "        \n",
    "        # Full Transfer Matrix: Out_Scale @ M2 @ Phase @ M1 @ In_Scale\n",
    "        # We perform M2 @ Diag @ M1 first\n",
    "        middle_term = m2 @ torch.diag_embed(h_diag) @ m1\n",
    "        \n",
    "        # Apply gains. Note: input gain applies to columns, output to rows.\n",
    "        # But middle_term is (Batch, 4, 4).\n",
    "        # We need to broadcast gains.\n",
    "        \n",
    "        # Convert gains to complex for multiplication\n",
    "        in_gain_c = in_gain.to(torch.complex128)\n",
    "        out_gain_c = out_gain.to(torch.complex128)\n",
    "        \n",
    "        # T_total = Out @ T_mid @ In\n",
    "        total_transfer = out_gain_c @ middle_term @ in_gain_c\n",
    "        \n",
    "        # Calculate Power Transmission\n",
    "        predict = torch.abs(total_transfer) ** 2\n",
    "        \n",
    "        # Add background noise (ensuring positivity)\n",
    "        predict = predict + torch.abs(self.background).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Select the correct input-output pair\n",
    "        # indx selects the Input Column (dim=2 in the (Batch, Row, Col) tensor)\n",
    "        # The gather index needs to be shaped (Batch, 4, 1) to pick columns\n",
    "        \n",
    "        # Original code used gather on dim=1. Let's stick to that logic, assuming\n",
    "        # the tensor structure matches your data prep.\n",
    "        # Typically: T_ij is Output i from Input j.\n",
    "        # So we want to gather along dim=2 (Input Columns) using indx.\n",
    "        # However, your original code used dim=1. I will respect that, but be aware \n",
    "        # that implies `indx` corresponds to rows.\n",
    "        \n",
    "        predict = torch.gather(predict, dim=1, index=indx)\n",
    "        \n",
    "        return predict.squeeze(1)\n",
    "    \n",
    "    def get_full_matrix(self, x, bottom_heaters=True):\n",
    "        alpha = self.get_constrained_alpha()\n",
    "        alpha_val = alpha.to(x.device).flip(0)\n",
    "        beta_val = self.beta.to(x.device).flip(0)\n",
    "        h_0_val = self.h_0.to(x.device)\n",
    "        \n",
    "        h_list = h_0_val + (alpha_val @ x.T).T + (beta_val @ (x**2).T).T\n",
    "        \n",
    "        if bottom_heaters:\n",
    "            h_diag = torch.cat([torch.ones((x.shape[0], 1), dtype=torch.complex128, device=x.device), torch.exp(1j * h_list)], dim=1)\n",
    "        else:\n",
    "            h_diag = torch.cat([torch.exp(1j * h_list), torch.ones((x.shape[0], 1), dtype=torch.complex128, device=x.device)], dim=1)\n",
    "\n",
    "        m1 = self.m1()\n",
    "        m2 = self.m2()\n",
    "        \n",
    "        in_gain = torch.diag(torch.exp(self.in_log_eff)).to(torch.complex128)\n",
    "        out_gain = torch.diag(torch.exp(self.out_log_eff)).to(torch.complex128)\n",
    "        \n",
    "        middle_term = m2 @ torch.diag_embed(h_diag) @ m1\n",
    "        total_transfer = out_gain @ middle_term @ in_gain\n",
    "        \n",
    "        power_matrix = torch.abs(total_transfer) ** 2\n",
    "        power_matrix = power_matrix + torch.abs(self.background).view(1, 4, 1)\n",
    "        \n",
    "        return power_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5ec49",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "model = Structure().to(device)\n",
    "\n",
    "x_all = x_all.to(device)\n",
    "target = target.to(device)\n",
    "indx = indx.to(device)\n",
    "\n",
    "optimizer_adam = optim.AdamW(model.parameters(), lr=0.02, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_adam, T_max=300, eta_min=1e-4)\n",
    "\n",
    "print(\"Phase 1: Exploration with Mixing Initialization...\")\n",
    "pbar = tqdm(range(400))\n",
    "for step in pbar:\n",
    "    optimizer_adam.zero_grad()\n",
    "    pred = model(x_all, indx, bottom_heaters=single_layer)\n",
    "  \n",
    "    loss = nn.functional.mse_loss(pred, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        pbar.set_description(f\"AdamW Loss: {loss.item():.6f}\")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    model.parameters(), \n",
    "    lr=0.001, \n",
    "    max_iter=50, \n",
    "    history_size=100,  # Increased history for better curvature approx\n",
    "    line_search_fn=\"strong_wolfe\",\n",
    "    tolerance_grad=1e-9,\n",
    "    tolerance_change=1e-9\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 2: LBFGS Precision Fitting...\")\n",
    "def closure():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    pred = model(x_all, indx, bottom_heaters=True)\n",
    "    loss = nn.functional.mse_loss(pred, target)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "pbar_lbfgs = tqdm(range(100))\n",
    "best_loss = float('inf')\n",
    "\n",
    "for step in pbar_lbfgs:\n",
    "    loss = optimizer_lbfgs.step(closure)\n",
    "    pbar_lbfgs.set_description(f\"LBFGS Loss: {loss.item():.8f}\")\n",
    "    \n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "\n",
    "    if step > 20 and loss.item() > best_loss - 1e-9:\n",
    "        print(\"Converged (Loss plateau).\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Best Loss: {best_loss}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    alpha_final = model.get_constrained_alpha()\n",
    "    print(\"\\nFinal Alpha Matrix:\")\n",
    "    print(alpha_final)\n",
    "    print(\"\\nConstraint Checks:\")\n",
    "    print(f\"a00 ({alpha_final[0,0]:.4f}) > a11 ({alpha_final[1,1]:.4f}): {alpha_final[0,0] > alpha_final[1,1]}\")\n",
    "    print(f\"a11 ({alpha_final[1,1]:.4f}) > a22 ({alpha_final[2,2]:.4f}): {alpha_final[1,1] > alpha_final[2,2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ff377",
   "metadata": {},
   "source": [
    "# Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912603eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_all, indx)\n",
    "\n",
    "df['ch1_pred'] = pred[:, 0].detach().cpu().numpy()\n",
    "df['ch2_pred'] = pred[:, 1].detach().cpu().numpy()\n",
    "df['ch3_pred'] = pred[:, 2].detach().cpu().numpy()\n",
    "df['ch4_pred'] = pred[:, 3].detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(25, 20))\n",
    "for j in range(4):\n",
    "    for i in range(3):\n",
    "        df_part = df[(df['in_ch']==j) &  (df['heater']==i)]\n",
    "        for col_true, col_pred, color in zip(\n",
    "            ['ch1', 'ch2', 'ch3', 'ch4'],\n",
    "            ['ch1_pred', 'ch2_pred', 'ch3_pred', 'ch4_pred'],\n",
    "            ['blue', 'green', 'yellow', 'red'],\n",
    "        ):\n",
    "            ax[i][j].plot(\n",
    "                df_part['current'], df_part[col_true],\n",
    "                label=f\"{col_true}, msmt\", color=color, alpha=1\n",
    "            )\n",
    "            ax[i][j].scatter(\n",
    "                df_part['current'], df_part[col_pred], marker='.', s=50,\n",
    "                color=color,label=f\"{col_true}, pred\"\n",
    "            )\n",
    "        ax[i][j].set_title(f'in_ch={j+1}, heater={i+14}',fontsize=16)\n",
    "        ax[i][j].set_xlabel('current (a.u.)', fontsize=16)\n",
    "        ax[i][j].set_ylabel('normalized power', fontsize=16)\n",
    "        ax[i][j].grid()\n",
    "ax[0][0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57ed64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
